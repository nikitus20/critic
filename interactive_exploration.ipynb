{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeltaBench Interactive Exploration Notebook\n",
    "\n",
    "**Comprehensive playground for exploring the DeltaBench dataset and testing DirectCritic vs PedCOT approaches**\n",
    "\n",
    "This notebook provides:\n",
    "- üìä **Dataset exploration** with interactive sampling\n",
    "- üîç **Annotation viewer** showing all parts of examples\n",
    "- ü§ñ **Critic testing** for both DirectCritic and PedCOT\n",
    "- ‚öñÔ∏è **Comparative analysis** between approaches\n",
    "- üéØ **Deep dive tools** for understanding error patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "Load all necessary libraries and initialize the DeltaBench framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "# DeltaBench framework\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from src import (\n",
    "    DeltaBenchDataset, DeltaBenchEvaluator, \n",
    "    DirectCritic, PedCoTCritic, CriticFactory, create_critic,\n",
    "    display_example, display_critic_comparison, summarize_results\n",
    ")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üîß Make sure your OPENAI_API_KEY is set in your environment or .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the DeltaBench dataset and display basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = DeltaBenchDataset()\n",
    "data = dataset.load_jsonl('data/Deltabench_v1.jsonl')\n",
    "\n",
    "print(f\"üìä Dataset loaded: {len(data)} examples\")\n",
    "print(f\"üìÅ File path: {dataset.file_path}\")\n",
    "\n",
    "# Basic statistics\n",
    "if data:\n",
    "    # Task distribution\n",
    "    task_counts = Counter(ex.get('task_l1', 'unknown') for ex in data)\n",
    "    print(\"\\nüìà Task Distribution:\")\n",
    "    for task, count in task_counts.most_common():\n",
    "        print(f\"   {task}: {count} ({count/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # Error statistics\n",
    "    error_examples = sum(1 for ex in data if ex.get('reason_error_section_numbers') or ex.get('reason_unuseful_section_numbers'))\n",
    "    print(f\"\\nüö® Examples with errors: {error_examples} ({error_examples/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # Sample first example keys\n",
    "    print(f\"\\nüîë Example structure (keys): {list(data[0].keys())}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Exploration Functions\n",
    "\n",
    "Interactive functions for sampling and exploring examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_examples(n: int = 5, task_type: Optional[str] = None, has_errors: bool = True, \n",
    "                   min_sections: int = 3, max_sections: int = 100) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Smart sampling of examples with filtering options.\n",
    "    \n",
    "    Args:\n",
    "        n: Number of examples to sample\n",
    "        task_type: Filter by task type ('math', 'code', etc.)\n",
    "        has_errors: Whether to include only examples with errors\n",
    "        min_sections: Minimum number of reasoning sections\n",
    "        max_sections: Maximum number of reasoning sections\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled examples\n",
    "    \"\"\"\n",
    "    filtered_data = data.copy()\n",
    "    \n",
    "    # Filter by task type\n",
    "    if task_type:\n",
    "        filtered_data = [ex for ex in filtered_data if ex.get('task_l1') == task_type]\n",
    "    \n",
    "    # Filter by error presence\n",
    "    if has_errors:\n",
    "        filtered_data = [ex for ex in filtered_data \n",
    "                        if ex.get('reason_error_section_numbers') or ex.get('reason_unuseful_section_numbers')]\n",
    "    \n",
    "    # Filter by section count (approximate)\n",
    "    def count_sections(example):\n",
    "        content = example.get('sections_content', '') or example.get('section_content', '') or example.get('long_cot', '')\n",
    "        # Count \"section\" patterns\n",
    "        import re\n",
    "        sections = re.findall(r'section\\s*\\d+', content, re.IGNORECASE)\n",
    "        return max(len(sections), len(content.split('\\n\\n'))) if content else 0\n",
    "    \n",
    "    filtered_data = [ex for ex in filtered_data \n",
    "                    if min_sections <= count_sections(ex) <= max_sections]\n",
    "    \n",
    "    if len(filtered_data) < n:\n",
    "        print(f\"‚ö†Ô∏è Only {len(filtered_data)} examples match filters (requested {n})\")\n",
    "        n = len(filtered_data)\n",
    "    \n",
    "    sampled = random.sample(filtered_data, min(n, len(filtered_data)))\n",
    "    \n",
    "    print(f\"üé≤ Sampled {len(sampled)} examples\")\n",
    "    if task_type:\n",
    "        print(f\"   üìã Task type: {task_type}\")\n",
    "    print(f\"   üö® Has errors: {has_errors}\")\n",
    "    print(f\"   üìè Section range: {min_sections}-{max_sections}\")\n",
    "    \n",
    "    return sampled\n",
    "\n",
    "def get_task_types():\n",
    "    \"\"\"Get all available task types from the dataset.\"\"\"\n",
    "    return sorted(set(ex.get('task_l1', 'unknown') for ex in data))\n",
    "\n",
    "def get_example_by_id(example_id: str) -> Optional[Dict]:\n",
    "    \"\"\"Find example by ID.\"\"\"\n",
    "    for ex in data:\n",
    "        if ex.get('id') == example_id:\n",
    "            return ex\n",
    "    return None\n",
    "\n",
    "# Test the sampling function\n",
    "print(\"Available task types:\", get_task_types())\n",
    "sample_examples(3, task_type='math')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rich Annotation Viewer\n",
    "\n",
    "Display complete example details with all annotations and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_full_example(example: Dict, show_reasoning: bool = True, show_sections: bool = True):\n",
    "    \"\"\"\n",
    "    Display a complete example with rich formatting and all annotations.\n",
    "    \n",
    "    Args:\n",
    "        example: The example dictionary\n",
    "        show_reasoning: Whether to show the detailed reasoning\n",
    "        show_sections: Whether to break down sections\n",
    "    \"\"\"\n",
    "    # Header with metadata\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"border: 2px solid #4CAF50; padding: 15px; margin: 10px 0; border-radius: 8px; background-color: #f9f9f9;\">\n",
    "        <h3 style=\"color: #2E7D32; margin-top: 0;\">üìã Example Details</h3>\n",
    "        <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 10px; margin-bottom: 10px;\">\n",
    "            <div><strong>ID:</strong> {example.get('id', 'N/A')}</div>\n",
    "            <div><strong>Origin:</strong> {example.get('origin', 'N/A')}</div>\n",
    "            <div><strong>Task L1:</strong> {example.get('task_l1', 'N/A')}</div>\n",
    "            <div><strong>Task L2:</strong> {example.get('task_l2', 'N/A')}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Question\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"border-left: 4px solid #2196F3; padding: 15px; margin: 10px 0; background-color: #f0f8ff;\">\n",
    "        <h4 style=\"color: #1976D2; margin-top: 0;\">‚ùì Question</h4>\n",
    "        <p style=\"line-height: 1.6;\">{example.get('question', 'N/A')}</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Answer and correctness\n",
    "    final_correct = example.get('final_correct', 'unknown')\n",
    "    correct_color = '#4CAF50' if final_correct else '#F44336' if final_correct is False else '#FF9800'\n",
    "    correct_icon = '‚úÖ' if final_correct else '‚ùå' if final_correct is False else '‚ùì'\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"border-left: 4px solid {correct_color}; padding: 15px; margin: 10px 0; background-color: #fafafa;\">\n",
    "        <h4 style=\"color: {correct_color}; margin-top: 0;\">{correct_icon} Final Answer</h4>\n",
    "        <p><strong>Answer:</strong> {example.get('answer', 'N/A')}</p>\n",
    "        <p><strong>Correct:</strong> {final_correct}</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Error annotations\n",
    "    error_sections = example.get('reason_error_section_numbers', [])\n",
    "    unuseful_sections = example.get('reason_unuseful_section_numbers', [])\n",
    "    all_errors = sorted(set(error_sections + unuseful_sections))\n",
    "    \n",
    "    if all_errors:\n",
    "        display(HTML(f\"\"\"\n",
    "        <div style=\"border-left: 4px solid #F44336; padding: 15px; margin: 10px 0; background-color: #fff5f5;\">\n",
    "            <h4 style=\"color: #D32F2F; margin-top: 0;\">üö® Error Annotations</h4>\n",
    "            <p><strong>Error sections:</strong> {error_sections}</p>\n",
    "            <p><strong>Unuseful sections:</strong> {unuseful_sections}</p>\n",
    "            <p><strong>All problematic sections:</strong> {all_errors}</p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "    else:\n",
    "        display(HTML(f\"\"\"\n",
    "        <div style=\"border-left: 4px solid #4CAF50; padding: 15px; margin: 10px 0; background-color: #f0fff0;\">\n",
    "            <h4 style=\"color: #388E3C; margin-top: 0;\">‚úÖ No Errors Annotated</h4>\n",
    "            <p>This example has no annotated reasoning errors.</p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "    \n",
    "    # Reasoning content\n",
    "    if show_reasoning:\n",
    "        reasoning_content = (example.get('sections_content') or \n",
    "                           example.get('section_content') or \n",
    "                           example.get('long_cot') or \n",
    "                           \"No reasoning content available\")\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <div style=\"border-left: 4px solid #9C27B0; padding: 15px; margin: 10px 0; background-color: #fdf7ff;\">\n",
    "            <h4 style=\"color: #7B1FA2; margin-top: 0;\">üß† Reasoning Content</h4>\n",
    "            <div style=\"max-height: 400px; overflow-y: auto; padding: 10px; background-color: white; border-radius: 4px; font-family: monospace; white-space: pre-wrap; line-height: 1.4;\">{reasoning_content}</div>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "    \n",
    "    # Section breakdown\n",
    "    if show_sections and 'sections' in example:\n",
    "        sections = example['sections']\n",
    "        display(HTML(\"<h4 style='color: #FF9800; margin-top: 20px;'>üìë Section Breakdown</h4>\"))\n",
    "        \n",
    "        for i, section in enumerate(sections, 1):\n",
    "            section_color = '#ffebee' if i in all_errors else '#f9f9f9'\n",
    "            section_border = '#F44336' if i in all_errors else '#ddd'\n",
    "            error_indicator = 'üö® ' if i in all_errors else ''\n",
    "            \n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"border: 1px solid {section_border}; padding: 10px; margin: 5px 0; background-color: {section_color}; border-radius: 4px;\">\n",
    "                <strong>{error_indicator}Section {i}:</strong> {section.get('description', 'No description')}<br>\n",
    "                <strong>Type:</strong> {section.get('section_type', 'Unknown')}<br>\n",
    "                <div style=\"margin-top: 8px; font-family: monospace; background-color: white; padding: 8px; border-radius: 3px; max-height: 200px; overflow-y: auto;\">{section.get('content', 'No content')}</div>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "\n",
    "# Example usage widget\n",
    "def create_example_viewer_widget():\n",
    "    \"\"\"Create an interactive widget for viewing examples.\"\"\"\n",
    "    \n",
    "    # Sample some examples for the dropdown\n",
    "    sample_ex = sample_examples(10, has_errors=True)\n",
    "    example_options = [(f\"{ex.get('task_l1', 'unknown')} - {ex.get('id', 'no-id')[:8]}\", ex) for ex in sample_ex]\n",
    "    \n",
    "    def view_example(example, show_reasoning, show_sections):\n",
    "        display_full_example(example, show_reasoning, show_sections)\n",
    "    \n",
    "    return interactive(\n",
    "        view_example,\n",
    "        example=widgets.Dropdown(options=example_options, description='Example:'),\n",
    "        show_reasoning=widgets.Checkbox(value=True, description='Show reasoning'),\n",
    "        show_sections=widgets.Checkbox(value=True, description='Show sections')\n",
    "    )\n",
    "\n",
    "print(\"üé® Rich annotation viewer functions ready!\")\n",
    "print(\"üìù Use display_full_example(example) to view any example\")\n",
    "print(\"üéÆ Use create_example_viewer_widget() for interactive viewing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Critic Testing Interface\n",
    "\n",
    "Functions for testing both DirectCritic and PedCOT on examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_direct_critic(example: Dict, model: str = 'gpt-4o-mini', show_details: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Test DirectCritic on an example and display results.\n",
    "    \n",
    "    Args:\n",
    "        example: The example to test\n",
    "        model: Model to use for the critic\n",
    "        show_details: Whether to show detailed output\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with critic results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize DirectCritic\n",
    "        critic = create_critic('direct', model=model)\n",
    "        \n",
    "        # Get the question and reasoning\n",
    "        question = example.get('question', '')\n",
    "        model_output = (example.get('sections_content') or \n",
    "                       example.get('section_content') or \n",
    "                       example.get('long_cot', ''))\n",
    "        \n",
    "        if not question or not model_output:\n",
    "            display(HTML('<div style=\"color: red;\">‚ùå Missing question or reasoning content</div>'))\n",
    "            return {}\n",
    "        \n",
    "        # Run the critic\n",
    "        display(HTML('<div style=\"color: blue;\">üîÑ Running DirectCritic...</div>'))\n",
    "        \n",
    "        critic_output, token_info = critic.evaluate_reasoning(question, model_output)\n",
    "        \n",
    "        if not critic_output:\n",
    "            display(HTML('<div style=\"color: red;\">‚ùå Critic failed to generate output</div>'))\n",
    "            return {}\n",
    "        \n",
    "        # Parse the output\n",
    "        ground_truth = example.get('reason_error_section_numbers', []) + example.get('reason_unuseful_section_numbers', [])\n",
    "        result = critic.parse_output(critic_output, ground_truth)\n",
    "        \n",
    "        if show_details:\n",
    "            # Display results\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"border: 2px solid #2196F3; padding: 15px; margin: 10px 0; border-radius: 8px; background-color: #f0f8ff;\">\n",
    "                <h3 style=\"color: #1976D2; margin-top: 0;\">ü§ñ DirectCritic Results</h3>\n",
    "                \n",
    "                <div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-bottom: 15px;\">\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>Precision</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #4CAF50;\">{result.precision:.3f}</span>\n",
    "                    </div>\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>Recall</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #FF9800;\">{result.recall:.3f}</span>\n",
    "                    </div>\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>F1 Score</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #9C27B0;\">{result.f1_score:.3f}</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div style=\"margin-bottom: 15px;\">\n",
    "                    <strong>Ground Truth Errors:</strong> {ground_truth}<br>\n",
    "                    <strong>Predicted Errors:</strong> {result.predicted_error_sections}<br>\n",
    "                    <strong>Token Usage:</strong> {token_info.get('total_tokens', 0)} tokens\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Show raw critic output\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"border-left: 4px solid #2196F3; padding: 15px; margin: 10px 0; background-color: #fafafa;\">\n",
    "                <h4 style=\"color: #1976D2; margin-top: 0;\">üìÑ Raw Critic Output</h4>\n",
    "                <div style=\"background-color: white; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; max-height: 300px; overflow-y: auto;\">{critic_output}</div>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "        \n",
    "        return {\n",
    "            'critic_type': 'DirectCritic',\n",
    "            'result': result,\n",
    "            'token_info': token_info,\n",
    "            'raw_output': critic_output,\n",
    "            'ground_truth': ground_truth\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(HTML(f'<div style=\"color: red;\">‚ùå Error testing DirectCritic: {str(e)}</div>'))\n",
    "        return {}\n",
    "\n",
    "def test_direct_general_critic(example: Dict, model: str = 'gpt-4o-mini', show_details: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Test Direct General logic critic on an example and display results.\n",
    "    \n",
    "    Args:\n",
    "        example: The example to test\n",
    "        model: Model to use for the critic\n",
    "        show_details: Whether to show detailed output\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with critic results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Direct General critic using LLMCritic with direct_general prompt\n",
    "        from src.critic import LLMCritic\n",
    "        critic = LLMCritic(model_name=model, prompt_type=\"direct_general\")\n",
    "        \n",
    "        # Get the question and reasoning\n",
    "        question = example.get('question', '')\n",
    "        model_output = (example.get('sections_content') or \n",
    "                       example.get('section_content') or \n",
    "                       example.get('long_cot', ''))\n",
    "        \n",
    "        if not question or not model_output:\n",
    "            display(HTML('<div style=\"color: red;\">‚ùå Missing question or reasoning content</div>'))\n",
    "            return {}\n",
    "        \n",
    "        # Run the critic\n",
    "        display(HTML('<div style=\"color: #FF5722;\">üîÑ Running Direct General Logic Critic...</div>'))\n",
    "        \n",
    "        critic_output, token_info = critic.evaluate_reasoning(question, model_output)\n",
    "        \n",
    "        if not critic_output:\n",
    "            display(HTML('<div style=\"color: red;\">‚ùå Critic failed to generate output</div>'))\n",
    "            return {}\n",
    "        \n",
    "        # Parse the output\n",
    "        ground_truth = example.get('reason_error_section_numbers', []) + example.get('reason_unuseful_section_numbers', [])\n",
    "        result = critic.parse_output(critic_output, ground_truth)\n",
    "        \n",
    "        if show_details:\n",
    "            # Display results\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"border: 2px solid #FF5722; padding: 15px; margin: 10px 0; border-radius: 8px; background-color: #fff3e0;\">\n",
    "                <h3 style=\"color: #E64A19; margin-top: 0;\">üß† Direct General Logic Critic Results</h3>\n",
    "                \n",
    "                <div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-bottom: 15px;\">\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>Precision</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #4CAF50;\">{result.precision:.3f}</span>\n",
    "                    </div>\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>Recall</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #FF9800;\">{result.recall:.3f}</span>\n",
    "                    </div>\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>F1 Score</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #9C27B0;\">{result.f1_score:.3f}</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div style=\"margin-bottom: 15px;\">\n",
    "                    <strong>Ground Truth Errors:</strong> {ground_truth}<br>\n",
    "                    <strong>Predicted Errors:</strong> {result.predicted_error_sections}<br>\n",
    "                    <strong>Token Usage:</strong> {token_info.get('total_tokens', 0)} tokens<br>\n",
    "                    <strong>Analysis Focus:</strong> Cross-section logic flow & pattern recognition\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Show raw critic output\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"border-left: 4px solid #FF5722; padding: 15px; margin: 10px 0; background-color: #fafafa;\">\n",
    "                <h4 style=\"color: #E64A19; margin-top: 0;\">üìÑ Raw Logic Critic Output</h4>\n",
    "                <div style=\"background-color: white; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; max-height: 300px; overflow-y: auto;\">{critic_output}</div>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "        \n",
    "        return {\n",
    "            'critic_type': 'DirectGeneralCritic',\n",
    "            'result': result,\n",
    "            'token_info': token_info,\n",
    "            'raw_output': critic_output,\n",
    "            'ground_truth': ground_truth\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(HTML(f'<div style=\"color: red;\">‚ùå Error testing Direct General critic: {str(e)}</div>'))\n",
    "        return {}\n",
    "\n",
    "def test_pedcot_critic(example: Dict, model: str = 'gpt-4o-mini', show_details: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Test PedCOT critic on an example with detailed two-stage breakdown.\n",
    "    \n",
    "    Args:\n",
    "        example: The example to test\n",
    "        model: Model to use for the critic\n",
    "        show_details: Whether to show detailed output\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with critic results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize PedCOT critic\n",
    "        critic = create_critic('pedcot', model=model)\n",
    "        \n",
    "        # Get the question and reasoning\n",
    "        question = example.get('question', '')\n",
    "        model_output = (example.get('sections_content') or \n",
    "                       example.get('section_content') or \n",
    "                       example.get('long_cot', ''))\n",
    "        \n",
    "        if not question or not model_output:\n",
    "            display(HTML('<div style=\"color: red;\">‚ùå Missing question or reasoning content</div>'))\n",
    "            return {}\n",
    "        \n",
    "        # Run the critic\n",
    "        display(HTML('<div style=\"color: purple;\">üîÑ Running PedCOT Critic (Two-Stage Process)...</div>'))\n",
    "        \n",
    "        critic_output, token_info = critic.evaluate_reasoning(question, model_output)\n",
    "        \n",
    "        if not critic_output:\n",
    "            display(HTML('<div style=\"color: red;\">‚ùå PedCOT critic failed to generate output</div>'))\n",
    "            return {}\n",
    "        \n",
    "        # Parse the output\n",
    "        ground_truth = example.get('reason_error_section_numbers', []) + example.get('reason_unuseful_section_numbers', [])\n",
    "        result = critic.parse_output(critic_output, ground_truth)\n",
    "        \n",
    "        if show_details:\n",
    "            # Display results\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"border: 2px solid #9C27B0; padding: 15px; margin: 10px 0; border-radius: 8px; background-color: #fdf7ff;\">\n",
    "                <h3 style=\"color: #7B1FA2; margin-top: 0;\">üéì PedCOT Critic Results</h3>\n",
    "                \n",
    "                <div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-bottom: 15px;\">\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>Precision</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #4CAF50;\">{result.precision:.3f}</span>\n",
    "                    </div>\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>Recall</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #FF9800;\">{result.recall:.3f}</span>\n",
    "                    </div>\n",
    "                    <div style=\"text-align: center; padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                        <strong>F1 Score</strong><br>\n",
    "                        <span style=\"font-size: 1.5em; color: #9C27B0;\">{result.f1_score:.3f}</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div style=\"margin-bottom: 15px;\">\n",
    "                    <strong>Ground Truth Errors:</strong> {ground_truth}<br>\n",
    "                    <strong>Predicted Errors:</strong> {result.predicted_error_sections}<br>\n",
    "                    <strong>Token Usage:</strong> {token_info.get('total_tokens', 0)} tokens<br>\n",
    "                    <strong>Domain:</strong> {token_info.get('domain', 'unknown')}<br>\n",
    "                    <strong>Stages:</strong> {token_info.get('num_stages', 0)} LLM calls\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Show raw critic output\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"border-left: 4px solid #9C27B0; padding: 15px; margin: 10px 0; background-color: #fafafa;\">\n",
    "                <h4 style=\"color: #7B1FA2; margin-top: 0;\">üìÑ Final PedCOT Output</h4>\n",
    "                <div style=\"background-color: white; padding: 10px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; max-height: 300px; overflow-y: auto;\">{critic_output}</div>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "        \n",
    "        return {\n",
    "            'critic_type': 'PedCoTCritic',\n",
    "            'result': result,\n",
    "            'token_info': token_info,\n",
    "            'raw_output': critic_output,\n",
    "            'ground_truth': ground_truth\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(HTML(f'<div style=\"color: red;\">‚ùå Error testing PedCOT: {str(e)}</div>'))\n",
    "        return {}\n",
    "\n",
    "print(\"ü§ñ Critic testing functions ready!\")\n",
    "print(\"üîç Use test_direct_critic(example) to test DirectCritic\")\n",
    "print(\"üß† Use test_direct_general_critic(example) to test Direct General Logic Critic\")\n",
    "print(\"üéì Use test_pedcot_critic(example) to test PedCOT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis\n",
    "\n",
    "Side-by-side comparison of both critics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_critics(example: Dict, model: str = 'gpt-4o-mini') -> Dict:\n",
    "    \"\"\"\n",
    "    Run all three critics on the same example and compare results.\n",
    "    \n",
    "    Args:\n",
    "        example: The example to test\n",
    "        model: Model to use for all critics\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"text-align: center; margin: 20px 0;\">\n",
    "        <h2 style=\"color: #333;\">‚öñÔ∏è Three-Way Critic Comparison</h2>\n",
    "        <p style=\"color: #666;\">Testing DirectCritic, Direct General, and PedCOT on the same example</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Test DirectCritic\n",
    "    display(HTML('<h3 style=\"color: #2196F3;\">ü§ñ DirectCritic Test</h3>'))\n",
    "    direct_result = test_direct_critic(example, model, show_details=True)\n",
    "    \n",
    "    # Test Direct General\n",
    "    display(HTML('<h3 style=\"color: #FF5722;\">üß† Direct General Logic Critic Test</h3>'))\n",
    "    direct_general_result = test_direct_general_critic(example, model, show_details=True)\n",
    "    \n",
    "    # Test PedCOT\n",
    "    display(HTML('<h3 style=\"color: #9C27B0;\">üéì PedCOT Test</h3>'))\n",
    "    pedcot_result = test_pedcot_critic(example, model, show_details=True)\n",
    "    \n",
    "    # Comparison summary\n",
    "    if direct_result and direct_general_result and pedcot_result:\n",
    "        direct_metrics = direct_result['result']\n",
    "        direct_general_metrics = direct_general_result['result']\n",
    "        pedcot_metrics = pedcot_result['result']\n",
    "        \n",
    "        comparison_data = {\n",
    "            'Metric': ['Precision', 'Recall', 'F1 Score', 'Predicted Errors', 'Token Usage'],\n",
    "            'DirectCritic': [\n",
    "                f\"{direct_metrics.precision:.3f}\",\n",
    "                f\"{direct_metrics.recall:.3f}\",\n",
    "                f\"{direct_metrics.f1_score:.3f}\",\n",
    "                str(direct_metrics.predicted_error_sections),\n",
    "                str(direct_result['token_info'].get('total_tokens', 0))\n",
    "            ],\n",
    "            'Direct General': [\n",
    "                f\"{direct_general_metrics.precision:.3f}\",\n",
    "                f\"{direct_general_metrics.recall:.3f}\",\n",
    "                f\"{direct_general_metrics.f1_score:.3f}\",\n",
    "                str(direct_general_metrics.predicted_error_sections),\n",
    "                str(direct_general_result['token_info'].get('total_tokens', 0))\n",
    "            ],\n",
    "            'PedCOT': [\n",
    "                f\"{pedcot_metrics.precision:.3f}\",\n",
    "                f\"{pedcot_metrics.recall:.3f}\",\n",
    "                f\"{pedcot_metrics.f1_score:.3f}\",\n",
    "                str(pedcot_metrics.predicted_error_sections),\n",
    "                str(pedcot_result['token_info'].get('total_tokens', 0))\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <div style=\"border: 2px solid #FF9800; padding: 15px; margin: 20px 0; border-radius: 8px; background-color: #fff8e1;\">\n",
    "            <h3 style=\"color: #F57C00; margin-top: 0;\">üìä Three-Way Comparison Summary</h3>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "        \n",
    "        display(comparison_df)\n",
    "        \n",
    "        # Winner analysis\n",
    "        f1_scores = {\n",
    "            'DirectCritic': direct_metrics.f1_score,\n",
    "            'Direct General': direct_general_metrics.f1_score,\n",
    "            'PedCOT': pedcot_metrics.f1_score\n",
    "        }\n",
    "        f1_winner = max(f1_scores.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        token_usage = {\n",
    "            'DirectCritic': direct_result['token_info'].get('total_tokens', 0),\n",
    "            'Direct General': direct_general_result['token_info'].get('total_tokens', 0),\n",
    "            'PedCOT': pedcot_result['token_info'].get('total_tokens', 0)\n",
    "        }\n",
    "        efficiency_winner = min(token_usage.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <div style=\"border-left: 4px solid #4CAF50; padding: 15px; margin: 10px 0; background-color: #f0fff0;\">\n",
    "            <h4 style=\"color: #388E3C; margin-top: 0;\">üèÜ Analysis</h4>\n",
    "            <p><strong>F1 Score Winner:</strong> {f1_winner} ({f1_scores[f1_winner]:.3f})</p>\n",
    "            <p><strong>Efficiency Winner:</strong> {efficiency_winner} ({token_usage[efficiency_winner]} tokens)</p>\n",
    "            <p><strong>Ground Truth:</strong> {direct_result['ground_truth']}</p>\n",
    "            <p><strong>F1 Scores:</strong> DirectCritic: {f1_scores['DirectCritic']:.3f}, Direct General: {f1_scores['Direct General']:.3f}, PedCOT: {f1_scores['PedCOT']:.3f}</p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "    \n",
    "    return {\n",
    "        'direct_result': direct_result,\n",
    "        'direct_general_result': direct_general_result,\n",
    "        'pedcot_result': pedcot_result,\n",
    "        'example_id': example.get('id', 'unknown')\n",
    "    }\n",
    "\n",
    "def batch_comparison(examples: List[Dict], model: str = 'gpt-4o-mini') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run batch comparison on multiple examples with all three critics.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of examples to test\n",
    "        model: Model to use\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comparison results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing example {i+1}/{len(examples)}: {example.get('id', 'unknown')[:8]}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Test all three critics (without detailed display)\n",
    "            direct_result = test_direct_critic(example, model, show_details=False)\n",
    "            direct_general_result = test_direct_general_critic(example, model, show_details=False)\n",
    "            pedcot_result = test_pedcot_critic(example, model, show_details=False)\n",
    "            \n",
    "            if direct_result and direct_general_result and pedcot_result:\n",
    "                results.append({\n",
    "                    'example_id': example.get('id', 'unknown'),\n",
    "                    'task_type': example.get('task_l1', 'unknown'),\n",
    "                    'ground_truth': direct_result['ground_truth'],\n",
    "                    'direct_precision': direct_result['result'].precision,\n",
    "                    'direct_recall': direct_result['result'].recall,\n",
    "                    'direct_f1': direct_result['result'].f1_score,\n",
    "                    'direct_tokens': direct_result['token_info'].get('total_tokens', 0),\n",
    "                    'direct_predictions': direct_result['result'].predicted_error_sections,\n",
    "                    'direct_general_precision': direct_general_result['result'].precision,\n",
    "                    'direct_general_recall': direct_general_result['result'].recall,\n",
    "                    'direct_general_f1': direct_general_result['result'].f1_score,\n",
    "                    'direct_general_tokens': direct_general_result['token_info'].get('total_tokens', 0),\n",
    "                    'direct_general_predictions': direct_general_result['result'].predicted_error_sections,\n",
    "                    'pedcot_precision': pedcot_result['result'].precision,\n",
    "                    'pedcot_recall': pedcot_result['result'].recall,\n",
    "                    'pedcot_f1': pedcot_result['result'].f1_score,\n",
    "                    'pedcot_tokens': pedcot_result['token_info'].get('total_tokens', 0),\n",
    "                    'pedcot_predictions': pedcot_result['result'].predicted_error_sections,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing example {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"‚öñÔ∏è Comparative analysis functions ready!\")\n",
    "print(\"üîÑ Use compare_critics(example) for three-way comparison\")\n",
    "print(\"üìä Use batch_comparison(examples) for multiple examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Playground\n",
    "\n",
    "Widgets for interactive exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive example viewer\n",
    "print(\"üéÆ Interactive Example Viewer\")\n",
    "print(\"Use the widget below to explore examples:\")\n",
    "create_example_viewer_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Start Playground\n",
    "\n",
    "Ready-to-use cells for immediate exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a few examples to play with\n",
    "print(\"üé≤ Sampling examples for exploration...\")\n",
    "playground_examples = sample_examples(3, task_type='math', has_errors=True)\n",
    "\n",
    "print(f\"\\nüìã Got {len(playground_examples)} examples to explore\")\n",
    "for i, ex in enumerate(playground_examples):\n",
    "    print(f\"   {i+1}. {ex.get('task_l1', 'unknown')} - {ex.get('id', 'no-id')[:8]}\")\n",
    "\n",
    "# Store first example for easy access\n",
    "if playground_examples:\n",
    "    current_example = playground_examples[0]\n",
    "    print(f\"\\n‚úÖ Current example set to: {current_example.get('id', 'unknown')[:8]}\")\n",
    "    print(\"üí° Use the cells below to explore this example\")\n",
    "else:\n",
    "    print(\"‚ùå No examples found. Check dataset loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the current example\n",
    "if 'current_example' in locals():\n",
    "    print(\"üìã Displaying current example...\")\n",
    "    display_full_example(current_example, show_reasoning=True, show_sections=True)\n",
    "else:\n",
    "    print(\"‚ùå No current example set. Run the cell above first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DirectCritic on current example\n",
    "if 'current_example' in locals():\n",
    "    print(\"ü§ñ Testing DirectCritic...\")\n",
    "    direct_test_result = test_direct_critic(current_example, show_details=True)\n",
    "else:\n",
    "    print(\"‚ùå No current example set. Run the sampling cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Direct General Logic Critic on current example\n",
    "if 'current_example' in locals():\n",
    "    print(\"üß† Testing Direct General Logic Critic...\")\n",
    "    direct_general_test_result = test_direct_general_critic(current_example, show_details=True)\n",
    "else:\n",
    "    print(\"‚ùå No current example set. Run the sampling cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PedCOT on current example\n",
    "if 'current_example' in locals():\n",
    "    print(\"üéì Testing PedCOT...\")\n",
    "    pedcot_test_result = test_pedcot_critic(current_example, show_details=True)\n",
    "else:\n",
    "    print(\"‚ùå No current example set. Run the sampling cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare all three critics\n",
    "if 'current_example' in locals():\n",
    "    print(\"‚öñÔ∏è Comparing all three critics...\")\n",
    "    comparison_result = compare_critics(current_example)\n",
    "else:\n",
    "    print(\"‚ùå No current example set. Run the sampling cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch comparison (uncomment to run - this will use API calls)\n",
    "# batch_examples = sample_examples(5, task_type='math', has_errors=True)\n",
    "# print(f\"Running batch comparison on {len(batch_examples)} examples...\")\n",
    "# batch_results = batch_comparison(batch_examples)\n",
    "# display(batch_results)\n",
    "\n",
    "print(\"üö´ Batch analysis is commented out to prevent accidental API usage\")\n",
    "print(\"üí° Uncomment the lines above to run batch comparison\")\n",
    "print(\"‚ö†Ô∏è Note: This will use OpenAI API tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Exploration\n",
    "\n",
    "Custom exploration functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom exploration - modify as needed\n",
    "\n",
    "def explore_by_task_type(task_type: str, n_examples: int = 3):\n",
    "    \"\"\"\n",
    "    Explore examples from a specific task type.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Exploring {task_type} examples...\")\n",
    "    examples = sample_examples(n_examples, task_type=task_type, has_errors=True)\n",
    "    \n",
    "    for i, ex in enumerate(examples):\n",
    "        print(f\"\\n{'='*20} Example {i+1} {'='*20}\")\n",
    "        display_full_example(ex, show_reasoning=False, show_sections=False)\n",
    "\n",
    "def find_complex_examples(min_errors: int = 3):\n",
    "    \"\"\"\n",
    "    Find examples with multiple errors.\n",
    "    \"\"\"\n",
    "    complex_examples = []\n",
    "    for ex in data:\n",
    "        error_count = len(ex.get('reason_error_section_numbers', [])) + len(ex.get('reason_unuseful_section_numbers', []))\n",
    "        if error_count >= min_errors:\n",
    "            complex_examples.append((ex, error_count))\n",
    "    \n",
    "    # Sort by error count\n",
    "    complex_examples.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"üéØ Found {len(complex_examples)} examples with {min_errors}+ errors\")\n",
    "    \n",
    "    return [ex for ex, _ in complex_examples[:10]]  # Return top 10\n",
    "\n",
    "# Available task types\n",
    "print(\"üìã Available task types:\", get_task_types())\n",
    "print(\"\\nüí° Use explore_by_task_type('math') to explore math examples\")\n",
    "print(\"üí° Use find_complex_examples(3) to find examples with 3+ errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ready to Explore!\n",
    "\n",
    "This notebook is now fully set up for interactive exploration of the DeltaBench dataset and testing all three critic approaches: **DirectCritic**, **Direct General Logic Critic**, and **PedCOT**.\n",
    "\n",
    "### Quick Start Guide:\n",
    "\n",
    "1. **üìä Data Exploration**: Use the sampling cells to get examples\n",
    "2. **üîç View Examples**: Use `display_full_example(example)` to see all annotations\n",
    "3. **ü§ñ Test Critics**: Use individual test functions or three-way comparison\n",
    "4. **‚öñÔ∏è Compare**: Use `compare_critics(example)` for side-by-side comparison of all three\n",
    "5. **üéÆ Interactive**: Use the widgets for GUI-based exploration\n",
    "\n",
    "### Available Critics:\n",
    "- **ü§ñ DirectCritic**: Original DeltaBench approach (section-by-section evaluation)\n",
    "- **üß† Direct General**: Advanced logic critic with cross-section analysis and pattern recognition\n",
    "- **üéì PedCOT**: Pedagogical Chain-of-Thought approach (two-stage process)\n",
    "\n",
    "### Key Functions Available:\n",
    "- `sample_examples(n, task_type, has_errors)` - Smart sampling\n",
    "- `display_full_example(example)` - Rich display with annotations\n",
    "- `test_direct_critic(example)` - Test DirectCritic approach\n",
    "- `test_direct_general_critic(example)` - Test Direct General Logic Critic\n",
    "- `test_pedcot_critic(example)` - Test PedCOT approach\n",
    "- `compare_critics(example)` - Three-way comparison\n",
    "- `batch_comparison(examples)` - Multiple example analysis\n",
    "\n",
    "### Direct General Logic Critic Features:\n",
    "- **Cross-section logic analysis** - tracks reasoning flow between sections\n",
    "- **Pattern recognition** - identifies specific error types (Unjustified Generalization, Invalid Simplification, etc.)\n",
    "- **Enhanced output format** - includes Error Type, Location, Explanation, and Impact\n",
    "- **Holistic evaluation** - considers both individual sections and overall logical flow\n",
    "\n",
    "**‚ö†Ô∏è Note**: Testing critics will use OpenAI API tokens. Make sure your API key is set!\n",
    "\n",
    "**üöÄ Happy Exploring with All Three Critics!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
